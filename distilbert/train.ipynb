{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "314a225b-a360-4d34-a308-37428c04efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import  AutoModelForSequenceClassification, AutoTokenizer\n",
    "import os\n",
    "os.environ['WANDB_MODE'] = 'disabled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5aff05d-cca2-4354-b836-11ce61b96e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c0bb407-6f2f-4c39-b222-1bb907625f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 234944 entries, 0 to 234943\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    234944 non-null  object\n",
      " 1   labels  234944 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e5bde53-aeee-4172-87b1-5f2af64f999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased\" # Define which pre-trained model we will be using\n",
    "classifier = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3) # Get the classifier\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint) # Get the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f297c63-c58a-4d57-8c9c-76210d4e4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_eval = train_test_split(df, train_size=0.8, stratify=df.labels, random_state=42) # Stratified splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13ecc0a8-cfd9-474f-a122-871b5179f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    \"eval\": Dataset.from_pandas(df_eval)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a71017f-6c2f-416b-9f31-5ba0fc15d217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Dict:\n",
      " DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels', '__index_level_0__'],\n",
      "        num_rows: 187955\n",
      "    })\n",
      "    eval: Dataset({\n",
      "        features: ['text', 'labels', '__index_level_0__'],\n",
      "        num_rows: 46989\n",
      "    })\n",
      "})\n",
      "\n",
      "\n",
      "Train's features:\n",
      " {'text': Value(dtype='string', id=None), 'labels': Value(dtype='int64', id=None), '__index_level_0__': Value(dtype='int64', id=None)}\n",
      "\n",
      "\n",
      "First row of Train:\n",
      " {'text': \"Question: Does the packaging emphasize child safety? Answer: Yes, it mentions 'safe for children with rounded edges,' but lacks official safety certifications.\", 'labels': 1, '__index_level_0__': 63701}\n"
     ]
    }
   ],
   "source": [
    "# Check the datasets\n",
    "print(\"Dataset Dict:\\n\", raw_datasets)\n",
    "print(\"\\n\\nTrain's features:\\n\", raw_datasets[\"train\"].features)\n",
    "print(\"\\n\\nFirst row of Train:\\n\", raw_datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c995aac-1bcb-4070-8111-0dcdd804e321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187955/187955 [00:04<00:00, 40847.43 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46989/46989 [00:01<00:00, 43081.45 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 187955\n",
      "    })\n",
      "    eval: Dataset({\n",
      "        features: ['text', 'labels', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 46989\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text, and truncate the text if it exceed the tokenizer maximum length. Batched=True to tokenize multiple texts at the same time.\n",
    "tokenized_datasets = raw_datasets.map(lambda dataset: tokenizer(dataset['text'], truncation=True), batched=True)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b267d28e-8ac6-4e21-9069-6071d1bc1a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Question: Does the packaging emphasize child safety? Answer: Yes, it mentions 'safe for children with rounded edges,' but lacks official safety certifications.\", 'labels': 1, '__index_level_0__': 63701, 'input_ids': [101, 3160, 1024, 2515, 1996, 14793, 17902, 2775, 3808, 1029, 3437, 1024, 2748, 1010, 2009, 9704, 1005, 3647, 2005, 2336, 2007, 8352, 7926, 1010, 1005, 2021, 14087, 2880, 3808, 10618, 2015, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac9e9945-80c2-4b98-b8b9-87e453e64e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 187955\n",
      "    })\n",
      "    eval: Dataset({\n",
      "        features: ['labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 46989\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"__index_level_0__\"])\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37084163-a261-4790-b7c1-c800e268e285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nightstalker/miniconda3/envs/pytorch/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/nightstalker/miniconda3/envs/pytorch/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Padding for batch of data that will be fed into model for training\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Training args \n",
    "training_args = TrainingArguments(\"train-checkpoints\", \n",
    "                                  num_train_epochs=10, \n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  weight_decay=5e-4, \n",
    "                                  per_device_train_batch_size=64,\n",
    "                                  per_device_eval_batch_size=64,\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  fp16=True,\n",
    "                                  load_best_model_at_end=True)\n",
    "\n",
    "# Metric for validation error\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Computes accuracy, F1, precision, and recall for a given set of predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (obj): An object containing label_ids and predictions attributes.\n",
    "            - label_ids (array-like): A 1D array of true class labels.\n",
    "            - predictions (array-like): A 2D array where each row represents\n",
    "              an observation, and each column represents the probability of\n",
    "              that observation belonging to a certain class.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the following metrics:\n",
    "            - Accuracy (float): The proportion of correctly classified instances.\n",
    "            - F1 (float): The macro F1 score, which is the harmonic mean of precision\n",
    "              and recall. Macro averaging calculates the metric independently for\n",
    "              each class and then takes the average.\n",
    "            - Precision (float): The macro precision, which is the number of true\n",
    "              positives divided by the sum of true positives and false positives.\n",
    "            - Recall (float): The macro recall, which is the number of true positives\n",
    "              divided by the sum of true positives and false negatives.\n",
    "    \"\"\"\n",
    "    # Extract true labels from the input object\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    # Obtain predicted class labels by finding the column index with the maximum probability\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    # Compute macro precision, recall, and F1 score using sklearn's precision_recall_fscore_support function\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "\n",
    "    # Calculate the accuracy score using sklearn's accuracy_score function\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    # Return the computed metrics as a dictionary\n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "    }\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    classifier,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"eval\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "100f94d6-94f9-43db-908c-c964463e3104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29370' max='29370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29370/29370 45:15, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.999957</td>\n",
       "      <td>0.999957</td>\n",
       "      <td>0.999957</td>\n",
       "      <td>0.999957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=29370, training_loss=0.0006539980641949714, metrics={'train_runtime': 2715.7721, 'train_samples_per_second': 692.087, 'train_steps_per_second': 10.815, 'total_flos': 2.7599966301479564e+16, 'train_loss': 0.0006539980641949714, 'epoch': 10.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the fine-tuning \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "266c6faa-fb71-473c-8063-dcf633253616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000     15667\n",
      "           1      1.000     1.000     1.000     15665\n",
      "           2      1.000     1.000     1.000     15657\n",
      "\n",
      "    accuracy                          1.000     46989\n",
      "   macro avg      1.000     1.000     1.000     46989\n",
      "weighted avg      1.000     1.000     1.000     46989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Make prediction on evaluation dataset\n",
    "y_pred = trainer.predict(tokenized_datasets[\"eval\"]).predictions\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "# Get the true labels\n",
    "y_true = tokenized_datasets[\"eval\"][\"labels\"]\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_true, y_pred, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
